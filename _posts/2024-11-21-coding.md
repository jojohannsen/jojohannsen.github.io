---
title: "Coding with a Software Design Agent"
date: 2024-11-21
categories: agents
---

While it's great to see AI assisted coding, it doesn't feel right.  I should be designing with the assistant, and occasionally coding functions that cannot be generated.

So the artifact we (myself and the Software Design Agent) work on should be the abstraction, service, concept.  We are working on a design, not code.   For code, we write functions only when the SDA and I determine it's necessary.

*Note: below when I say **we**, I mean myself and the Software Design Agent.*

Here is an example how that might work:

1. we are working on a plan-and-execute agent

   ```bash
   pip install langgraph-codegen
   ```

   ```bash
   lgcodegen plan_and_execute
   ```

   Outputs the graph architecture:

   ```
   LangGraph CodeGen v0.1.32
   # Plan and Execute Agent
   START(PlanExecute) => plan_step
   
   plan_step => execute_step
   
   execute_step => replan_step
   
   replan_step
     is_done => END
     => execute_step
   ```

2. At this point, we have the graph (a pattern), but the *state*, *nodes*, and *conditions* are only named, not defined.  However, we do know what functions are needed and their signatures.   Nodes take in state, and return state updates.  Conditions take in state, and return True/False.   **This is something the Software Design Agent should know.**

3. Since state (here named PlanExecute) is central, and we know that we should have some concept of what is in the state, and what that first node 'plan_step' should be doing.  Here's the information the SDA needs to know:

   - What sort of information do we provide?  Why are we using a plan-and-execute approach?  Do we need any tools during the planning phase?

   Here's what the SDA already knows.

   - The planning step requires an LLM.  The plan itself is structured output.  

   What we discuss, my answers in quotes:

   - what should the plan look like?   "just text for each step of the plan"
   - how do we ask for the plan?   "here's my prompt, let's come up with a few alternatives to try out"

4. We run the graph, with mock nodes for the incomplete parts.

   ```bash
   lgcodegen plan_and_execute --code
   python plan_and_execute/plan_and_execute.py
   ```

I'm trying to come up with #2 and #3 above, that Software Design Agent.  

One thing I'm trying is a suggestion from George, making the SDA from multiple obsessed-with-X agents, managed by a higher level agent for getting info from the lower-level agents and human, and evaluating multiple implementations.

At every step, the SDA has:

- things it needs to know
- mocks for incomplete code
- generated code
- Human code

The SDA and human discuss areas to work, and modify these sets as development proceeds.  At any point, there is a runnable implementation, tests, and evaluations (for comparing different implementations, prompts for example, and choosing initial baseline)

Hopefully this description isn't too abstract.  IDEs centered on code have a lot of ways a reasoning assistant can be integrated, and directly coding is an obvious option.   When cars first appeared, "horseless carriage" was an obvious name as well.  But they weren't just horseless carriages.  And my IDE with an AI isn't just a "humanless coder", even though it is for now.